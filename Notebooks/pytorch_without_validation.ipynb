{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26d986e3-be1c-49a1-8f6b-887720cf9877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# importing the neural network library\n",
    "import torch.nn as nn\n",
    "# to move the data forward in the function\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f4e0c-068e-4a6e-84bb-b76f08a3a6e0",
   "metadata": {},
   "source": [
    "<h3>Nueral Network Layout</h3>\n",
    "\n",
    "We will be using a neural network approach to classify. The approach consists of severaal fully connected layers where the first is features, followed by some hidden layers and finally an output layer.\n",
    "![nn_sample](https://www.qtravel.ai/wp-content/uploads/2023/07/sieci-neuronowe-grafika.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fb5dbcc-676b-4d8b-a8f4-8db806036911",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "p-value\n",
    "random_state_number = 42\n",
    "number_of_features = 1250\n",
    "iterations = 3500\n",
    "learning_rate = 2e-4\n",
    "testing_size  = 0.20\n",
    "number_of_neurons = 15    ==> 77.14285714285715%\n",
    "'''\n",
    "\n",
    "# Setting the model variables\n",
    "random_state_number = 42\n",
    "number_of_features = 1250\n",
    "iterations = 3500\n",
    "learning_rate = 2e-4\n",
    "testing_size  = 0.20\n",
    "number_of_neurons = 15\n",
    "# Creating a model class inheriting the nn module\n",
    "class Model(nn.Module):\n",
    "    ''' creating input layers that will have the fetures of the data\n",
    "     this will be sent to hidden layers to learn and finally this will \n",
    "     produce the classification as the output'''\n",
    "    # the number of features depend on the dataset, in this example we only have 4\n",
    "    # h1 and h2 are hidden layers and the numbers are number of neurons which are \n",
    "    # arbitrary for now. Out fetures is set to 3 as we have three classes in the model\n",
    "    #number_of_features = 1500#26707 # Number of gene IDs\n",
    "    number_of_outcomes = 2     # Possible outcomes\n",
    "\n",
    "    \n",
    "    def __init__(self, in_features = number_of_features, h1 = number_of_neurons , h2 = number_of_neurons, out_features = number_of_outcomes):\n",
    "        # calling the constructor of the superclass to instantiate nn.Module\n",
    "        super().__init__()\n",
    "        \n",
    "        # fc1 = fully connected layer 1\n",
    "        # we are moving the data between the layers\n",
    "        self.fc1 = nn.Linear(in_features , h1)\n",
    "        self.fc2 = nn.Linear(h1 , h2)\n",
    "        self.out = nn.Linear(h2 , out_features)\n",
    "\n",
    "# need to create a function that moves everything forward in the layers\n",
    "    def forward(self, x):\n",
    "        # rectified linear unit function that will only use positive output and set negatives to zero\n",
    "        x = F.relu(self.fc1(x)) # starting with layer 1\n",
    "        x = F.relu(self.fc2(x)) # moving to layer 2\n",
    "        x = self.out(x)         # moving to the output layer\n",
    "\n",
    "        return x                # returning the output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c6e6e6-05cd-474d-a1ae-40c9befa709c",
   "metadata": {},
   "source": [
    "Since neural networks rely on randomization, we will need to create a seed to allow regenerating the output. The number used here can be any number nut it will help us to regenerate  the output if we needed to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00aedcb-48c0-4bbf-a853-de4e74fbf1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(random_state_number) # can be any number\n",
    "# Creating an instance of out model to turn on the code above\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14df623-db5d-4008-8b58-704022394749",
   "metadata": {},
   "source": [
    "<h4>Loading the Data and Training the Model</h4>\n",
    "\n",
    "Now that our model is created, we can load the data and train the model on the data that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211d39f1-3d3f-4d3d-a4ac-37e0964f1293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000226571</th>\n",
       "      <th>ENSG00000230202</th>\n",
       "      <th>ENSG00000130202</th>\n",
       "      <th>ENSG00000286830</th>\n",
       "      <th>ENSG00000162383</th>\n",
       "      <th>ENSG00000167034</th>\n",
       "      <th>ENSG00000230021</th>\n",
       "      <th>ENSG00000232573</th>\n",
       "      <th>ENSG00000211804</th>\n",
       "      <th>ENSG00000132464</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000164707</th>\n",
       "      <th>ENSG00000143850</th>\n",
       "      <th>ENSG00000135211</th>\n",
       "      <th>ENSG00000211593</th>\n",
       "      <th>ENSG00000235194</th>\n",
       "      <th>ENSG00000228237</th>\n",
       "      <th>ENSG00000132196</th>\n",
       "      <th>ENSG00000168538</th>\n",
       "      <th>ENSG00000187953</th>\n",
       "      <th>Flare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.326576</td>\n",
       "      <td>4.884384</td>\n",
       "      <td>29.306303</td>\n",
       "      <td>20.758632</td>\n",
       "      <td>74.486855</td>\n",
       "      <td>111.119734</td>\n",
       "      <td>81.813430</td>\n",
       "      <td>9.768768</td>\n",
       "      <td>2.442192</td>\n",
       "      <td>6.105480</td>\n",
       "      <td>...</td>\n",
       "      <td>155.079189</td>\n",
       "      <td>8.547672</td>\n",
       "      <td>56.170415</td>\n",
       "      <td>15.874248</td>\n",
       "      <td>289.399747</td>\n",
       "      <td>94.024390</td>\n",
       "      <td>95.245486</td>\n",
       "      <td>382.203041</td>\n",
       "      <td>65.939183</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.114614</td>\n",
       "      <td>9.875074</td>\n",
       "      <td>28.802300</td>\n",
       "      <td>3.291691</td>\n",
       "      <td>9.875074</td>\n",
       "      <td>17.281380</td>\n",
       "      <td>35.385683</td>\n",
       "      <td>22.218917</td>\n",
       "      <td>31.271068</td>\n",
       "      <td>4.937537</td>\n",
       "      <td>...</td>\n",
       "      <td>143.188576</td>\n",
       "      <td>12.343843</td>\n",
       "      <td>62.542137</td>\n",
       "      <td>32.916914</td>\n",
       "      <td>432.034497</td>\n",
       "      <td>82.292285</td>\n",
       "      <td>101.219511</td>\n",
       "      <td>497.868325</td>\n",
       "      <td>60.073368</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.747243</td>\n",
       "      <td>2.731552</td>\n",
       "      <td>59.183626</td>\n",
       "      <td>10.015691</td>\n",
       "      <td>32.778624</td>\n",
       "      <td>11.836725</td>\n",
       "      <td>37.331210</td>\n",
       "      <td>23.673451</td>\n",
       "      <td>10.015691</td>\n",
       "      <td>25.494485</td>\n",
       "      <td>...</td>\n",
       "      <td>231.271401</td>\n",
       "      <td>5.463104</td>\n",
       "      <td>39.152245</td>\n",
       "      <td>214.882089</td>\n",
       "      <td>390.611933</td>\n",
       "      <td>115.635701</td>\n",
       "      <td>103.798975</td>\n",
       "      <td>542.668327</td>\n",
       "      <td>89.230698</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.579733</td>\n",
       "      <td>14.714193</td>\n",
       "      <td>11.252030</td>\n",
       "      <td>6.924326</td>\n",
       "      <td>74.436504</td>\n",
       "      <td>10.386489</td>\n",
       "      <td>25.966222</td>\n",
       "      <td>21.638519</td>\n",
       "      <td>15.579733</td>\n",
       "      <td>16.445274</td>\n",
       "      <td>...</td>\n",
       "      <td>161.856120</td>\n",
       "      <td>5.193244</td>\n",
       "      <td>64.915556</td>\n",
       "      <td>55.394608</td>\n",
       "      <td>442.291323</td>\n",
       "      <td>90.881779</td>\n",
       "      <td>129.831112</td>\n",
       "      <td>420.652804</td>\n",
       "      <td>92.612860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.880258</td>\n",
       "      <td>8.468792</td>\n",
       "      <td>103.036975</td>\n",
       "      <td>50.812755</td>\n",
       "      <td>8.468792</td>\n",
       "      <td>19.760516</td>\n",
       "      <td>47.989824</td>\n",
       "      <td>15.526120</td>\n",
       "      <td>7.057327</td>\n",
       "      <td>2.822931</td>\n",
       "      <td>...</td>\n",
       "      <td>193.370762</td>\n",
       "      <td>15.526120</td>\n",
       "      <td>52.224220</td>\n",
       "      <td>15.526120</td>\n",
       "      <td>389.564454</td>\n",
       "      <td>86.099390</td>\n",
       "      <td>93.156717</td>\n",
       "      <td>405.090574</td>\n",
       "      <td>46.578359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.139760</td>\n",
       "      <td>1.194524</td>\n",
       "      <td>20.306902</td>\n",
       "      <td>15.528807</td>\n",
       "      <td>8.361665</td>\n",
       "      <td>13.139760</td>\n",
       "      <td>58.531658</td>\n",
       "      <td>20.306902</td>\n",
       "      <td>3.583571</td>\n",
       "      <td>14.334284</td>\n",
       "      <td>...</td>\n",
       "      <td>143.342835</td>\n",
       "      <td>7.167142</td>\n",
       "      <td>50.169992</td>\n",
       "      <td>23.890473</td>\n",
       "      <td>401.359938</td>\n",
       "      <td>81.227607</td>\n",
       "      <td>112.285221</td>\n",
       "      <td>536.341108</td>\n",
       "      <td>66.893323</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.507277</td>\n",
       "      <td>10.507277</td>\n",
       "      <td>37.359206</td>\n",
       "      <td>5.837376</td>\n",
       "      <td>15.177177</td>\n",
       "      <td>15.177177</td>\n",
       "      <td>50.201433</td>\n",
       "      <td>18.679603</td>\n",
       "      <td>10.507277</td>\n",
       "      <td>5.837376</td>\n",
       "      <td>...</td>\n",
       "      <td>145.934399</td>\n",
       "      <td>12.842227</td>\n",
       "      <td>50.201433</td>\n",
       "      <td>46.699008</td>\n",
       "      <td>456.482800</td>\n",
       "      <td>108.575193</td>\n",
       "      <td>110.910143</td>\n",
       "      <td>505.516758</td>\n",
       "      <td>68.881036</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28.912215</td>\n",
       "      <td>181.449072</td>\n",
       "      <td>32.900106</td>\n",
       "      <td>7.975783</td>\n",
       "      <td>194.409720</td>\n",
       "      <td>3.987892</td>\n",
       "      <td>42.869836</td>\n",
       "      <td>16.948540</td>\n",
       "      <td>2.990919</td>\n",
       "      <td>9.969729</td>\n",
       "      <td>...</td>\n",
       "      <td>125.618588</td>\n",
       "      <td>13.957621</td>\n",
       "      <td>50.845619</td>\n",
       "      <td>10.966702</td>\n",
       "      <td>431.689276</td>\n",
       "      <td>118.639778</td>\n",
       "      <td>114.651886</td>\n",
       "      <td>482.534895</td>\n",
       "      <td>58.821402</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.932509</td>\n",
       "      <td>11.683840</td>\n",
       "      <td>41.424523</td>\n",
       "      <td>35.051520</td>\n",
       "      <td>5.310836</td>\n",
       "      <td>12.746007</td>\n",
       "      <td>45.673192</td>\n",
       "      <td>11.683840</td>\n",
       "      <td>4.248669</td>\n",
       "      <td>3.186502</td>\n",
       "      <td>...</td>\n",
       "      <td>227.303793</td>\n",
       "      <td>7.435171</td>\n",
       "      <td>44.611025</td>\n",
       "      <td>9.559505</td>\n",
       "      <td>421.680402</td>\n",
       "      <td>88.159882</td>\n",
       "      <td>129.584406</td>\n",
       "      <td>502.405113</td>\n",
       "      <td>60.543534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22.725222</td>\n",
       "      <td>6.197788</td>\n",
       "      <td>48.549338</td>\n",
       "      <td>23.758186</td>\n",
       "      <td>19.626328</td>\n",
       "      <td>16.527434</td>\n",
       "      <td>48.549338</td>\n",
       "      <td>11.362611</td>\n",
       "      <td>14.461505</td>\n",
       "      <td>85.736064</td>\n",
       "      <td>...</td>\n",
       "      <td>150.812836</td>\n",
       "      <td>20.659293</td>\n",
       "      <td>32.021904</td>\n",
       "      <td>26.857080</td>\n",
       "      <td>506.152669</td>\n",
       "      <td>85.736064</td>\n",
       "      <td>146.680978</td>\n",
       "      <td>423.515498</td>\n",
       "      <td>68.175666</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENSG00000226571  ENSG00000230202  ENSG00000130202  ENSG00000286830  \\\n",
       "0         7.326576         4.884384        29.306303        20.758632   \n",
       "1         4.114614         9.875074        28.802300         3.291691   \n",
       "2        12.747243         2.731552        59.183626        10.015691   \n",
       "3        15.579733        14.714193        11.252030         6.924326   \n",
       "4         9.880258         8.468792       103.036975        50.812755   \n",
       "5        13.139760         1.194524        20.306902        15.528807   \n",
       "6        10.507277        10.507277        37.359206         5.837376   \n",
       "7        28.912215       181.449072        32.900106         7.975783   \n",
       "8        15.932509        11.683840        41.424523        35.051520   \n",
       "9        22.725222         6.197788        48.549338        23.758186   \n",
       "\n",
       "   ENSG00000162383  ENSG00000167034  ENSG00000230021  ENSG00000232573  \\\n",
       "0        74.486855       111.119734        81.813430         9.768768   \n",
       "1         9.875074        17.281380        35.385683        22.218917   \n",
       "2        32.778624        11.836725        37.331210        23.673451   \n",
       "3        74.436504        10.386489        25.966222        21.638519   \n",
       "4         8.468792        19.760516        47.989824        15.526120   \n",
       "5         8.361665        13.139760        58.531658        20.306902   \n",
       "6        15.177177        15.177177        50.201433        18.679603   \n",
       "7       194.409720         3.987892        42.869836        16.948540   \n",
       "8         5.310836        12.746007        45.673192        11.683840   \n",
       "9        19.626328        16.527434        48.549338        11.362611   \n",
       "\n",
       "   ENSG00000211804  ENSG00000132464  ...  ENSG00000164707  ENSG00000143850  \\\n",
       "0         2.442192         6.105480  ...       155.079189         8.547672   \n",
       "1        31.271068         4.937537  ...       143.188576        12.343843   \n",
       "2        10.015691        25.494485  ...       231.271401         5.463104   \n",
       "3        15.579733        16.445274  ...       161.856120         5.193244   \n",
       "4         7.057327         2.822931  ...       193.370762        15.526120   \n",
       "5         3.583571        14.334284  ...       143.342835         7.167142   \n",
       "6        10.507277         5.837376  ...       145.934399        12.842227   \n",
       "7         2.990919         9.969729  ...       125.618588        13.957621   \n",
       "8         4.248669         3.186502  ...       227.303793         7.435171   \n",
       "9        14.461505        85.736064  ...       150.812836        20.659293   \n",
       "\n",
       "   ENSG00000135211  ENSG00000211593  ENSG00000235194  ENSG00000228237  \\\n",
       "0        56.170415        15.874248       289.399747        94.024390   \n",
       "1        62.542137        32.916914       432.034497        82.292285   \n",
       "2        39.152245       214.882089       390.611933       115.635701   \n",
       "3        64.915556        55.394608       442.291323        90.881779   \n",
       "4        52.224220        15.526120       389.564454        86.099390   \n",
       "5        50.169992        23.890473       401.359938        81.227607   \n",
       "6        50.201433        46.699008       456.482800       108.575193   \n",
       "7        50.845619        10.966702       431.689276       118.639778   \n",
       "8        44.611025         9.559505       421.680402        88.159882   \n",
       "9        32.021904        26.857080       506.152669        85.736064   \n",
       "\n",
       "   ENSG00000132196  ENSG00000168538  ENSG00000187953  Flare  \n",
       "0        95.245486       382.203041        65.939183      3  \n",
       "1       101.219511       497.868325        60.073368      1  \n",
       "2       103.798975       542.668327        89.230698      1  \n",
       "3       129.831112       420.652804        92.612860      1  \n",
       "4        93.156717       405.090574        46.578359      1  \n",
       "5       112.285221       536.341108        66.893323      3  \n",
       "6       110.910143       505.516758        68.881036      3  \n",
       "7       114.651886       482.534895        58.821402      1  \n",
       "8       129.584406       502.405113        60.543534      1  \n",
       "9       146.680978       423.515498        68.175666      3  \n",
       "\n",
       "[10 rows x 1251 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt                        # for plotting\n",
    "import pandas as pd                                    # for dealing with dataframes\n",
    "import numpy as np                                     # to allow using numpy arrays below\n",
    "from sklearn.model_selection import train_test_split   # to allow splitting, training and testing\n",
    "\n",
    "# filtering function\n",
    "\n",
    "def selected_sample(filtering_set, original_data, number_of_ids):\n",
    "    selected_IDs = filtering_set[\"GeneID\"][0:number_of_ids].tolist()\n",
    "    selected_IDs.append(\"Flare\")\n",
    "    return original_data[selected_IDs] \n",
    "    \n",
    "# creating a url to load the data from the iris dataset\n",
    "#data_url = \"provide csv file's url here\"\n",
    "                    # replace this with url if importing one online\n",
    "\n",
    "'''\n",
    "    This is where we can change the dataset to train on a different data\n",
    "'''\n",
    "# Loading sorted gene ids:\n",
    "p_value_sorted = pd.read_csv(\"../Data/p_value_data.csv\")\n",
    "log2fc_sorted = pd.read_csv(\"../Data/log2fc_data.csv\")\n",
    "\n",
    "#loading all data\n",
    "all_data = pd.read_csv('../Data/processed_data.csv')   \n",
    "my_df = selected_sample(filtering_set = p_value_sorted, original_data = all_data, number_of_ids = number_of_features)  \n",
    "my_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9430b3-0e6a-4062-86c3-614842db72b1",
   "metadata": {},
   "source": [
    "To avoid any potential problems with data, we will encode the species column into numerical values as most machine learning approaches work better with numerical values. These values could then be encoded back to any class names that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e24166fa-5276-41e3-b7d3-81c07addf7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/60/jkn0_02s1lv9xfxqt4gzj2lw0000gn/T/ipykernel_58725/2233537459.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  my_df['Flare'] = my_df['Flare'].replace(3,0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENSG00000226571</th>\n",
       "      <th>ENSG00000230202</th>\n",
       "      <th>ENSG00000130202</th>\n",
       "      <th>ENSG00000286830</th>\n",
       "      <th>ENSG00000162383</th>\n",
       "      <th>ENSG00000167034</th>\n",
       "      <th>ENSG00000230021</th>\n",
       "      <th>ENSG00000232573</th>\n",
       "      <th>ENSG00000211804</th>\n",
       "      <th>ENSG00000132464</th>\n",
       "      <th>...</th>\n",
       "      <th>ENSG00000164707</th>\n",
       "      <th>ENSG00000143850</th>\n",
       "      <th>ENSG00000135211</th>\n",
       "      <th>ENSG00000211593</th>\n",
       "      <th>ENSG00000235194</th>\n",
       "      <th>ENSG00000228237</th>\n",
       "      <th>ENSG00000132196</th>\n",
       "      <th>ENSG00000168538</th>\n",
       "      <th>ENSG00000187953</th>\n",
       "      <th>Flare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.326576</td>\n",
       "      <td>4.884384</td>\n",
       "      <td>29.306303</td>\n",
       "      <td>20.758632</td>\n",
       "      <td>74.486855</td>\n",
       "      <td>111.119734</td>\n",
       "      <td>81.813430</td>\n",
       "      <td>9.768768</td>\n",
       "      <td>2.442192</td>\n",
       "      <td>6.105480</td>\n",
       "      <td>...</td>\n",
       "      <td>155.079189</td>\n",
       "      <td>8.547672</td>\n",
       "      <td>56.170415</td>\n",
       "      <td>15.874248</td>\n",
       "      <td>289.399747</td>\n",
       "      <td>94.024390</td>\n",
       "      <td>95.245486</td>\n",
       "      <td>382.203041</td>\n",
       "      <td>65.939183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.114614</td>\n",
       "      <td>9.875074</td>\n",
       "      <td>28.802300</td>\n",
       "      <td>3.291691</td>\n",
       "      <td>9.875074</td>\n",
       "      <td>17.281380</td>\n",
       "      <td>35.385683</td>\n",
       "      <td>22.218917</td>\n",
       "      <td>31.271068</td>\n",
       "      <td>4.937537</td>\n",
       "      <td>...</td>\n",
       "      <td>143.188576</td>\n",
       "      <td>12.343843</td>\n",
       "      <td>62.542137</td>\n",
       "      <td>32.916914</td>\n",
       "      <td>432.034497</td>\n",
       "      <td>82.292285</td>\n",
       "      <td>101.219511</td>\n",
       "      <td>497.868325</td>\n",
       "      <td>60.073368</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.747243</td>\n",
       "      <td>2.731552</td>\n",
       "      <td>59.183626</td>\n",
       "      <td>10.015691</td>\n",
       "      <td>32.778624</td>\n",
       "      <td>11.836725</td>\n",
       "      <td>37.331210</td>\n",
       "      <td>23.673451</td>\n",
       "      <td>10.015691</td>\n",
       "      <td>25.494485</td>\n",
       "      <td>...</td>\n",
       "      <td>231.271401</td>\n",
       "      <td>5.463104</td>\n",
       "      <td>39.152245</td>\n",
       "      <td>214.882089</td>\n",
       "      <td>390.611933</td>\n",
       "      <td>115.635701</td>\n",
       "      <td>103.798975</td>\n",
       "      <td>542.668327</td>\n",
       "      <td>89.230698</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.579733</td>\n",
       "      <td>14.714193</td>\n",
       "      <td>11.252030</td>\n",
       "      <td>6.924326</td>\n",
       "      <td>74.436504</td>\n",
       "      <td>10.386489</td>\n",
       "      <td>25.966222</td>\n",
       "      <td>21.638519</td>\n",
       "      <td>15.579733</td>\n",
       "      <td>16.445274</td>\n",
       "      <td>...</td>\n",
       "      <td>161.856120</td>\n",
       "      <td>5.193244</td>\n",
       "      <td>64.915556</td>\n",
       "      <td>55.394608</td>\n",
       "      <td>442.291323</td>\n",
       "      <td>90.881779</td>\n",
       "      <td>129.831112</td>\n",
       "      <td>420.652804</td>\n",
       "      <td>92.612860</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.880258</td>\n",
       "      <td>8.468792</td>\n",
       "      <td>103.036975</td>\n",
       "      <td>50.812755</td>\n",
       "      <td>8.468792</td>\n",
       "      <td>19.760516</td>\n",
       "      <td>47.989824</td>\n",
       "      <td>15.526120</td>\n",
       "      <td>7.057327</td>\n",
       "      <td>2.822931</td>\n",
       "      <td>...</td>\n",
       "      <td>193.370762</td>\n",
       "      <td>15.526120</td>\n",
       "      <td>52.224220</td>\n",
       "      <td>15.526120</td>\n",
       "      <td>389.564454</td>\n",
       "      <td>86.099390</td>\n",
       "      <td>93.156717</td>\n",
       "      <td>405.090574</td>\n",
       "      <td>46.578359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13.139760</td>\n",
       "      <td>1.194524</td>\n",
       "      <td>20.306902</td>\n",
       "      <td>15.528807</td>\n",
       "      <td>8.361665</td>\n",
       "      <td>13.139760</td>\n",
       "      <td>58.531658</td>\n",
       "      <td>20.306902</td>\n",
       "      <td>3.583571</td>\n",
       "      <td>14.334284</td>\n",
       "      <td>...</td>\n",
       "      <td>143.342835</td>\n",
       "      <td>7.167142</td>\n",
       "      <td>50.169992</td>\n",
       "      <td>23.890473</td>\n",
       "      <td>401.359938</td>\n",
       "      <td>81.227607</td>\n",
       "      <td>112.285221</td>\n",
       "      <td>536.341108</td>\n",
       "      <td>66.893323</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.507277</td>\n",
       "      <td>10.507277</td>\n",
       "      <td>37.359206</td>\n",
       "      <td>5.837376</td>\n",
       "      <td>15.177177</td>\n",
       "      <td>15.177177</td>\n",
       "      <td>50.201433</td>\n",
       "      <td>18.679603</td>\n",
       "      <td>10.507277</td>\n",
       "      <td>5.837376</td>\n",
       "      <td>...</td>\n",
       "      <td>145.934399</td>\n",
       "      <td>12.842227</td>\n",
       "      <td>50.201433</td>\n",
       "      <td>46.699008</td>\n",
       "      <td>456.482800</td>\n",
       "      <td>108.575193</td>\n",
       "      <td>110.910143</td>\n",
       "      <td>505.516758</td>\n",
       "      <td>68.881036</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>28.912215</td>\n",
       "      <td>181.449072</td>\n",
       "      <td>32.900106</td>\n",
       "      <td>7.975783</td>\n",
       "      <td>194.409720</td>\n",
       "      <td>3.987892</td>\n",
       "      <td>42.869836</td>\n",
       "      <td>16.948540</td>\n",
       "      <td>2.990919</td>\n",
       "      <td>9.969729</td>\n",
       "      <td>...</td>\n",
       "      <td>125.618588</td>\n",
       "      <td>13.957621</td>\n",
       "      <td>50.845619</td>\n",
       "      <td>10.966702</td>\n",
       "      <td>431.689276</td>\n",
       "      <td>118.639778</td>\n",
       "      <td>114.651886</td>\n",
       "      <td>482.534895</td>\n",
       "      <td>58.821402</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>15.932509</td>\n",
       "      <td>11.683840</td>\n",
       "      <td>41.424523</td>\n",
       "      <td>35.051520</td>\n",
       "      <td>5.310836</td>\n",
       "      <td>12.746007</td>\n",
       "      <td>45.673192</td>\n",
       "      <td>11.683840</td>\n",
       "      <td>4.248669</td>\n",
       "      <td>3.186502</td>\n",
       "      <td>...</td>\n",
       "      <td>227.303793</td>\n",
       "      <td>7.435171</td>\n",
       "      <td>44.611025</td>\n",
       "      <td>9.559505</td>\n",
       "      <td>421.680402</td>\n",
       "      <td>88.159882</td>\n",
       "      <td>129.584406</td>\n",
       "      <td>502.405113</td>\n",
       "      <td>60.543534</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22.725222</td>\n",
       "      <td>6.197788</td>\n",
       "      <td>48.549338</td>\n",
       "      <td>23.758186</td>\n",
       "      <td>19.626328</td>\n",
       "      <td>16.527434</td>\n",
       "      <td>48.549338</td>\n",
       "      <td>11.362611</td>\n",
       "      <td>14.461505</td>\n",
       "      <td>85.736064</td>\n",
       "      <td>...</td>\n",
       "      <td>150.812836</td>\n",
       "      <td>20.659293</td>\n",
       "      <td>32.021904</td>\n",
       "      <td>26.857080</td>\n",
       "      <td>506.152669</td>\n",
       "      <td>85.736064</td>\n",
       "      <td>146.680978</td>\n",
       "      <td>423.515498</td>\n",
       "      <td>68.175666</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1251 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ENSG00000226571  ENSG00000230202  ENSG00000130202  ENSG00000286830  \\\n",
       "0         7.326576         4.884384        29.306303        20.758632   \n",
       "1         4.114614         9.875074        28.802300         3.291691   \n",
       "2        12.747243         2.731552        59.183626        10.015691   \n",
       "3        15.579733        14.714193        11.252030         6.924326   \n",
       "4         9.880258         8.468792       103.036975        50.812755   \n",
       "5        13.139760         1.194524        20.306902        15.528807   \n",
       "6        10.507277        10.507277        37.359206         5.837376   \n",
       "7        28.912215       181.449072        32.900106         7.975783   \n",
       "8        15.932509        11.683840        41.424523        35.051520   \n",
       "9        22.725222         6.197788        48.549338        23.758186   \n",
       "\n",
       "   ENSG00000162383  ENSG00000167034  ENSG00000230021  ENSG00000232573  \\\n",
       "0        74.486855       111.119734        81.813430         9.768768   \n",
       "1         9.875074        17.281380        35.385683        22.218917   \n",
       "2        32.778624        11.836725        37.331210        23.673451   \n",
       "3        74.436504        10.386489        25.966222        21.638519   \n",
       "4         8.468792        19.760516        47.989824        15.526120   \n",
       "5         8.361665        13.139760        58.531658        20.306902   \n",
       "6        15.177177        15.177177        50.201433        18.679603   \n",
       "7       194.409720         3.987892        42.869836        16.948540   \n",
       "8         5.310836        12.746007        45.673192        11.683840   \n",
       "9        19.626328        16.527434        48.549338        11.362611   \n",
       "\n",
       "   ENSG00000211804  ENSG00000132464  ...  ENSG00000164707  ENSG00000143850  \\\n",
       "0         2.442192         6.105480  ...       155.079189         8.547672   \n",
       "1        31.271068         4.937537  ...       143.188576        12.343843   \n",
       "2        10.015691        25.494485  ...       231.271401         5.463104   \n",
       "3        15.579733        16.445274  ...       161.856120         5.193244   \n",
       "4         7.057327         2.822931  ...       193.370762        15.526120   \n",
       "5         3.583571        14.334284  ...       143.342835         7.167142   \n",
       "6        10.507277         5.837376  ...       145.934399        12.842227   \n",
       "7         2.990919         9.969729  ...       125.618588        13.957621   \n",
       "8         4.248669         3.186502  ...       227.303793         7.435171   \n",
       "9        14.461505        85.736064  ...       150.812836        20.659293   \n",
       "\n",
       "   ENSG00000135211  ENSG00000211593  ENSG00000235194  ENSG00000228237  \\\n",
       "0        56.170415        15.874248       289.399747        94.024390   \n",
       "1        62.542137        32.916914       432.034497        82.292285   \n",
       "2        39.152245       214.882089       390.611933       115.635701   \n",
       "3        64.915556        55.394608       442.291323        90.881779   \n",
       "4        52.224220        15.526120       389.564454        86.099390   \n",
       "5        50.169992        23.890473       401.359938        81.227607   \n",
       "6        50.201433        46.699008       456.482800       108.575193   \n",
       "7        50.845619        10.966702       431.689276       118.639778   \n",
       "8        44.611025         9.559505       421.680402        88.159882   \n",
       "9        32.021904        26.857080       506.152669        85.736064   \n",
       "\n",
       "   ENSG00000132196  ENSG00000168538  ENSG00000187953  Flare  \n",
       "0        95.245486       382.203041        65.939183      0  \n",
       "1       101.219511       497.868325        60.073368      1  \n",
       "2       103.798975       542.668327        89.230698      1  \n",
       "3       129.831112       420.652804        92.612860      1  \n",
       "4        93.156717       405.090574        46.578359      1  \n",
       "5       112.285221       536.341108        66.893323      0  \n",
       "6       110.910143       505.516758        68.881036      0  \n",
       "7       114.651886       482.534895        58.821402      1  \n",
       "8       129.584406       502.405113        60.543534      1  \n",
       "9       146.680978       423.515498        68.175666      0  \n",
       "\n",
       "[10 rows x 1251 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# avoids downcasting warning as we are downcasting from an object (string) to a primitive data type\n",
    "# May need to use this based on flare numbers but not for now\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True) \n",
    "\n",
    "my_df['Flare'] = my_df['Flare'].replace(3,0)\n",
    "\n",
    "\n",
    "# Ensuring the datatype is an integer to avoid an error with y_train\n",
    "#my_df['species'] = my_df['species'].astype(np.int64)\n",
    "my_df.head(10)\n",
    "#print(my_df['species'].dtype)  # testing type -> should be int64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9603cc3-6a66-4517-b056-7005de10553f",
   "metadata": {},
   "source": [
    "<h3>Train, Test, and Split</h3>\n",
    "Now we want to split our dataset and train our model so that it can be able to classify new entires into one of the three classes based on the features that we get.\n",
    "\n",
    "First, we need to saxe the features on one side and the classification on another side. We will save all features in a variable called X and the classification or outcome in a variable called y. Having the X as uppercase and y as lowercase is just the codeing convention for such problems but it does not really matter if we made both uppercase or both lowercase.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "490205b8-063e-4db9-9d03-9c9be66a0439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X will ahve geneIDs and y will have flare/no flare\n",
    "X = my_df.drop('Flare', axis = 1).values # axis = 1 since it is a column\n",
    "y = my_df['Flare'].values\n",
    "\n",
    "# training, testing and splitting\n",
    "# using 80% of the data for training and 20 for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = testing_size, random_state = random_state_number) # last part is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce24f0f-00df-4378-bfe5-4b6bd6af593c",
   "metadata": {},
   "source": [
    "<h4>Convert to Tensors</h4>\n",
    "As pyTorch is all about tensors so we need the data to be converted to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb95622-abac-470f-9f01-7d36bd29b939",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_train = torch.LongTensor(y_train) # Using Long as these are 64 bit integers to match what we have above\n",
    "y_test = torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4af180a-7222-4aeb-bd34-999f04592372",
   "metadata": {},
   "source": [
    "<h4>Measuring Error</h4>\n",
    "To check how far off the prediction is from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0e0d703-0a88-4048-a0e8-959b84105bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking X_train for NaNs or infinities:\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "Checking y_train for NaNs or invalid values:\n",
      "tensor(0)\n",
      "tensor(0)\n",
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "'''\n",
    " Choosing an optimizer <- need to check later if there are better ones\n",
    " Setting the learning rate. If the error does not go down after a certain amount of \n",
    " epochs then we need to lower this. Parameters here are fc1, fc2, and output.\n",
    "'''\n",
    "#torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # the lower the rate = longer training time\n",
    "\n",
    "# double checking the parameters:\n",
    "model.parameters\n",
    "# X_train had nan values so I replaced them with zeros for now\n",
    "X_train = torch.nan_to_num(X_train, nan=0.0)\n",
    "X_test = torch.nan_to_num(X_test, nan=0.0)\n",
    "\n",
    "\n",
    "print(\"Checking X_train for NaNs or infinities:\")\n",
    "print(torch.isnan(X_train).sum())  # Should be 0 if no NaN values\n",
    "print(torch.isinf(X_train).sum())  # Should be 0 if no infinite values\n",
    "\n",
    "\n",
    "print(\"Checking y_train for NaNs or invalid values:\")\n",
    "print(torch.isnan(y_train).sum())  # Should be 0 if no NaN values\n",
    "print(torch.isinf(y_train).sum())  # Should be 0 if no infinite values\n",
    "print(torch.unique(y_train)) # Should be only [0, 1] for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b0412-6619-4554-87ba-e67b8c560c97",
   "metadata": {},
   "source": [
    "<h4>Training the Model</h4>\n",
    "We need to set up the number of epochs. An epoch is one run through all of our training data. Basically running all the trainig data through the entire netweork. We also want to keep track of the loss which should decrease overtime to show that the model is learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "454e568c-7bf6-4bfc-9ee3-4c83f4395166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 , Loss: 38.06991195678711\n",
      "Epoch: 100 , Loss: 0.6973076462745667\n",
      "Epoch: 200 , Loss: 0.40419405698776245\n",
      "Epoch: 300 , Loss: 0.21730880439281464\n",
      "Epoch: 400 , Loss: 0.13347841799259186\n",
      "Epoch: 500 , Loss: 0.06971614807844162\n",
      "Epoch: 600 , Loss: 0.04443449154496193\n",
      "Epoch: 700 , Loss: 0.030710384249687195\n",
      "Epoch: 800 , Loss: 0.022515200078487396\n",
      "Epoch: 900 , Loss: 0.01700417883694172\n",
      "Epoch: 1000 , Loss: 0.013295282609760761\n",
      "Epoch: 1100 , Loss: 0.010653112083673477\n",
      "Epoch: 1200 , Loss: 0.008699815720319748\n",
      "Epoch: 1300 , Loss: 0.007215939927846193\n",
      "Epoch: 1400 , Loss: 0.006054365076124668\n",
      "Epoch: 1500 , Loss: 0.005150021519511938\n",
      "Epoch: 1600 , Loss: 0.004424495156854391\n",
      "Epoch: 1700 , Loss: 0.0038494772743433714\n",
      "Epoch: 1800 , Loss: 0.003364274278283119\n",
      "Epoch: 1900 , Loss: 0.002959681674838066\n",
      "Epoch: 2000 , Loss: 0.0026197086554020643\n",
      "Epoch: 2100 , Loss: 0.0023358671460300684\n",
      "Epoch: 2200 , Loss: 0.002092033624649048\n",
      "Epoch: 2300 , Loss: 0.00187613011803478\n",
      "Epoch: 2400 , Loss: 0.001693467260338366\n",
      "Epoch: 2500 , Loss: 0.001538171898573637\n",
      "Epoch: 2600 , Loss: 0.001393043203279376\n",
      "Epoch: 2700 , Loss: 0.0012688300339505076\n",
      "Epoch: 2800 , Loss: 0.0011685218196362257\n",
      "Epoch: 2900 , Loss: 0.0010599133092910051\n",
      "Epoch: 3000 , Loss: 0.0009733291808515787\n",
      "Epoch: 3100 , Loss: 0.0008923408458940685\n",
      "Epoch: 3200 , Loss: 0.0008215208072215319\n",
      "Epoch: 3300 , Loss: 0.0007589060696773231\n",
      "Epoch: 3400 , Loss: 0.0006986790685914457\n"
     ]
    }
   ],
   "source": [
    "epochs = iterations # we can change this to optimize the learining\n",
    "losses = [] # to store loss overtime\n",
    "for i in range(epochs):\n",
    "    # trying to get predicted results by sending the training data through the network\n",
    "    y_pred = model.forward(X_train)\n",
    "\n",
    "    # Keeping track of the loss (error) using the predicted value vs the training value\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    # saving losses in the list to have a history to show how well we are learning\n",
    "    losses.append(loss.detach().numpy())\n",
    "\n",
    "    # Print data to see what is going on -  we will print every 10 iterations\n",
    "    if i%100 == 0:\n",
    "        print(f\"Epoch: {i} , Loss: {loss}\")\n",
    "\n",
    "    '''Do backporpagation to take the error rate moving forward and feed it back through the \n",
    "        network to fine tune the weights for the neural netweork '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    ''' Clears the old gradients from the previous step: if you don't zero out the gradients after each step, \n",
    "    the gradients from the previous batch will be added to the gradients of the current batch, which \n",
    "    will mess up the training process.'''\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bab2e554-2d62-4cda-8864-f484f0aa03fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Epoch')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting to see how we are learning\n",
    "\n",
    "plt.plot(range(epochs),losses)\n",
    "plt.ylabel(\"Loss Rate\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "#plt.xlim(1,290)\n",
    "#plt.ylim(0.68,0.69)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17bd9ec-528e-42dc-aa6f-71abd409cd6b",
   "metadata": {},
   "source": [
    "<h4>Evaluate and Test</h4>\n",
    "Here we will test the model to see if it was able to classify instances based on the model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9edd933-a08c-4ad2-b2b2-718fca81d695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5411)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad(): # turning off backpropagation to send only through the model not back\n",
    "    y_eval = model.forward(X_test) # X_test : features from our test set.\n",
    "    loss = criterion(y_eval , y_test)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3175b6d1-6e9a-4831-a0b8-78962eb84620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. tensor([50.4762, 50.1435]), Test was: 1\n",
      "1. tensor([31.4572, 46.6063]), Test was: 1\n",
      "2. tensor([32.6209, 39.1444]), Test was: 1\n",
      "3. tensor([63.4825, 61.8769]), Test was: 0\n",
      "4. tensor([48.3202, 61.2973]), Test was: 0\n",
      "5. tensor([30.0461, 50.8026]), Test was: 1\n",
      "6. tensor([42.4878, 40.9524]), Test was: 0\n",
      "7. tensor([47.4955, 53.4705]), Test was: 1\n",
      "8. tensor([53.3802, 47.6242]), Test was: 1\n",
      "9. tensor([46.3643, 47.7583]), Test was: 1\n",
      "10. tensor([58.2325, 55.2453]), Test was: 0\n",
      "11. tensor([40.3010, 37.4085]), Test was: 0\n",
      "12. tensor([46.9971, 56.2067]), Test was: 1\n",
      "13. tensor([42.8026, 40.3173]), Test was: 0\n",
      "14. tensor([49.9207, 40.6282]), Test was: 0\n",
      "15. tensor([46.9275, 45.4035]), Test was: 0\n",
      "16. tensor([58.7323, 52.3646]), Test was: 0\n",
      "17. tensor([42.7522, 47.1219]), Test was: 1\n",
      "18. tensor([32.5131, 39.3951]), Test was: 0\n",
      "19. tensor([35.9538, 52.5645]), Test was: 1\n",
      "20. tensor([41.9028, 52.7625]), Test was: 1\n",
      "21. tensor([51.8158, 42.0941]), Test was: 0\n",
      "22. tensor([38.8291, 51.1175]), Test was: 1\n",
      "23. tensor([39.6625, 41.5040]), Test was: 1\n",
      "24. tensor([36.7807, 49.4333]), Test was: 1\n",
      "25. tensor([31.0635, 43.7940]), Test was: 0\n",
      "26. tensor([41.5197, 48.0269]), Test was: 1\n",
      "27. tensor([48.8214, 46.2801]), Test was: 0\n",
      "28. tensor([52.0117, 42.3511]), Test was: 0\n",
      "29. tensor([45.2745, 40.9648]), Test was: 0\n",
      "30. tensor([41.1047, 39.6475]), Test was: 0\n",
      "31. tensor([46.8141, 49.5069]), Test was: 0\n",
      "32. tensor([52.4129, 48.6477]), Test was: 1\n",
      "33. tensor([46.7309, 40.0437]), Test was: 1\n",
      "34. tensor([54.7045, 51.3251]), Test was: 0\n",
      "Correct percentage:77.14285714285715%\n"
     ]
    }
   ],
   "source": [
    "correct = 0 \n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(X_test):\n",
    "        y_val = model.forward(data)\n",
    "        print(f'{i}. {y_val}, Test was: {y_test[i]}' )\n",
    "        if y_val.argmax().item() == y_test[i]:\n",
    "            correct+=1\n",
    "print(f'Correct percentage:{correct/len(y_test) *100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6c9bc-90fa-4525-b1f6-186afbb333f8",
   "metadata": {},
   "source": [
    "<h3>Feeding New Data</h3>\n",
    "In the next section, we can feed new data into the model and try to make the model predict/classify the instance based on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b258a1-cb62-49db-9b61-f9f760337dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4201a1f6-2c87-465c-b2c4-db65a39e5407",
   "metadata": {},
   "source": [
    "<h3>Saving the Neural Network Model</h3>\n",
    "\n",
    "Now that or model is trained and tested, we can sace it to use it later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d50ad8e-79ca-4d0a-a275-a92e9af29bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'gout_model_p_value.pt') # will save the weights and biases of the model into a dictionar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd623e-25c6-48da-89ea-7874e3bb694b",
   "metadata": {},
   "source": [
    "Now that our model is saved, we can load it as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74432492-fecb-4943-97fe-7940a7eba717",
   "metadata": {},
   "source": [
    "### new_model = Model()\n",
    "new_model.load_state_dict(torch.load('iris_test_model.pt'))\n",
    "new_model.eval()\n",
    "\n",
    "### to use the model, we will use the new_model to predict/ classify instead of model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
